{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a73195b-5797-4373-abd7-6676189e9e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456edc36-0f8f-4dee-b32a-a31d68e53b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Dataset Preparation\n",
    "DATASET_PATH = \"C:\\Users\\heman\\hand gesture\"  # Replace with your dataset path\n",
    "IMAGE_SIZE = (64, 64)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Use ImageDataGenerator for loading and augmenting the dataset\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255.0,  # Normalize pixel values\n",
    "    rotation_range=20,    # Random rotation\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # Split for training and validation\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    DATASET_PATH,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    DATASET_PATH,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"validation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0b85a1-d0ac-440d-8216-0a6e230dc29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Build the CNN Model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(train_generator.num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a03033-c4c9-4094-bbdc-adf927ec51b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train the Model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=20,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96647504-1ec3-4c68-be29-10e992887c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Evaluate the Model\n",
    "val_loss, val_acc = model.evaluate(val_generator, verbose=1)\n",
    "print(f\"Validation Accuracy: {val_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b2c3a5-4572-457a-afb8-226202eca2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Save the Model\n",
    "model.save(\"hand_gesture_model.h5\")\n",
    "print(\"Model saved as hand_gesture_model.h5.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45827c5c-91fb-40a7-a094-9bfa20c7eecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Visualize Training History\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e039e8-44fd-4e83-bae0-6ceac74ffc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Real-Time Gesture Recognition (Optional)\n",
    "def real_time_gesture_recognition(model, image_size=IMAGE_SIZE):\n",
    "    cap = cv2.VideoCapture(0)  # Open the webcam\n",
    "    labels = list(train_generator.class_indices.keys())  # Get class labels\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Preprocess the frame\n",
    "        img = cv2.resize(frame, image_size)\n",
    "        img = img / 255.0\n",
    "        img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "        \n",
    "        # Make prediction\n",
    "        predictions = model.predict(img)\n",
    "        predicted_class = labels[np.argmax(predictions)]\n",
    "        \n",
    "        # Display the prediction\n",
    "        cv2.putText(frame, f\"Gesture: {predicted_class}\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.imshow(\"Hand Gesture Recognition\", frame)\n",
    "        \n",
    "        # Break the loop on 'q' key press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
